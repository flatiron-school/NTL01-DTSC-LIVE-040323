{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45225d96ef8db436",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ML Fundamentals Checkpoint\n",
    "\n",
    "This checkpoint is designed to test your understanding of the content from the Machine Learning Fundamentals Cumulative Lab.\n",
    "\n",
    "Specifically, this will cover:\n",
    "\n",
    "* Performing a train-test split to evaluate model performance on unseen data\n",
    "* Applying appropriate preprocessing steps to training and test data\n",
    "* Identifying overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80e2de554ec6f219",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Your Task: Build and Evaluate a Ridge Regression Model to Predict Home Prices\n",
    "\n",
    "### Data Understanding\n",
    "\n",
    "You will be using the Ames Housing dataset, modeling the `SalePrice` based on all other numeric features of the dataset. You can view the `data_description.txt` file for explanations of these variables if desired, but the specific feature descriptions can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0eab44802b65ea1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>60</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>953</td>\n",
       "      <td>953</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>20</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>790</td>\n",
       "      <td>163</td>\n",
       "      <td>589</td>\n",
       "      <td>1542</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>70</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>877</td>\n",
       "      <td>1152</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>49</td>\n",
       "      <td>1029</td>\n",
       "      <td>0</td>\n",
       "      <td>1078</td>\n",
       "      <td>...</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>20</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>830</td>\n",
       "      <td>290</td>\n",
       "      <td>136</td>\n",
       "      <td>1256</td>\n",
       "      <td>...</td>\n",
       "      <td>736</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "Id                                                                             \n",
       "1             60     8450            7            5       2003          2003   \n",
       "2             20     9600            6            8       1976          1976   \n",
       "3             60    11250            7            5       2001          2002   \n",
       "4             70     9550            7            5       1915          1970   \n",
       "5             60    14260            8            5       2000          2000   \n",
       "...          ...      ...          ...          ...        ...           ...   \n",
       "1456          60     7917            6            5       1999          2000   \n",
       "1457          20    13175            6            6       1978          1988   \n",
       "1458          70     9042            7            9       1941          2006   \n",
       "1459          20     9717            5            6       1950          1996   \n",
       "1460          20     9937            5            6       1965          1965   \n",
       "\n",
       "      BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  ...  WoodDeckSF  \\\n",
       "Id                                                    ...               \n",
       "1            706           0        150          856  ...           0   \n",
       "2            978           0        284         1262  ...         298   \n",
       "3            486           0        434          920  ...           0   \n",
       "4            216           0        540          756  ...           0   \n",
       "5            655           0        490         1145  ...         192   \n",
       "...          ...         ...        ...          ...  ...         ...   \n",
       "1456           0           0        953          953  ...           0   \n",
       "1457         790         163        589         1542  ...         349   \n",
       "1458         275           0        877         1152  ...           0   \n",
       "1459          49        1029          0         1078  ...         366   \n",
       "1460         830         290        136         1256  ...         736   \n",
       "\n",
       "      OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
       "Id                                                                            \n",
       "1              61              0          0            0         0        0   \n",
       "2               0              0          0            0         0        0   \n",
       "3              42              0          0            0         0        0   \n",
       "4              35            272          0            0         0        0   \n",
       "5              84              0          0            0         0        0   \n",
       "...           ...            ...        ...          ...       ...      ...   \n",
       "1456           40              0          0            0         0        0   \n",
       "1457            0              0          0            0         0        0   \n",
       "1458           60              0          0            0         0     2500   \n",
       "1459            0            112          0            0         0        0   \n",
       "1460           68              0          0            0         0        0   \n",
       "\n",
       "      MoSold  YrSold  SalePrice  \n",
       "Id                               \n",
       "1          2    2008     208500  \n",
       "2          5    2007     181500  \n",
       "3          9    2008     223500  \n",
       "4          2    2006     140000  \n",
       "5         12    2008     250000  \n",
       "...      ...     ...        ...  \n",
       "1456       8    2007     175000  \n",
       "1457       2    2010     210000  \n",
       "1458       5    2010     266500  \n",
       "1459       4    2010     142125  \n",
       "1460       6    2008     147500  \n",
       "\n",
       "[1460 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "\n",
    "# Read in CSV file\n",
    "df = pd.read_csv(\"ames.csv\", index_col=0)\n",
    "# Keep only numeric data\n",
    "df = df.select_dtypes(include=\"number\")\n",
    "# Keep only columns with no missing values\n",
    "df = df.dropna(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-927990ad603957cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Modeling\n",
    "\n",
    "You will apply a **predictive** modeling process using scikit-learn. That means that you are trying to build a model with the best performance on predicting the target (`SalePrice`) using the features of unseen data.\n",
    "\n",
    "For this reason you will first perform a **train-test split**, so that you are fitting the model using the training dataset and evaluating the model using the testing dataset.\n",
    "\n",
    "You will also report model **metrics** in terms of both r-squared and RMSE, for both the train and test data, in order to interpret your model performance.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### 1. Perform a Train-Test Split\n",
    "\n",
    "#### 2. Preprocess Data\n",
    "\n",
    "#### 3. Fit a `Ridge` Model\n",
    "\n",
    "#### 4. Evaluate the Model Performance\n",
    "\n",
    "#### 5. Interpret the Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5667f384accd25f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Perform a Train-Test Split\n",
    "\n",
    "This step has two parts. First, separate `df` into `X` and `y`.\n",
    "\n",
    "* `X` should be a pandas `DataFrame` containing all columns of `df` except for the target\n",
    "* `y` should be a pandas `Series` containing just the target\n",
    "\n",
    "The target is `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4893c1f1208ec61",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step1.1\n",
    "# Replace None with appropriate code\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the type and shape of X\n",
    "assert type(X) == pd.DataFrame\n",
    "assert X.shape == (1460, 33)\n",
    "\n",
    "# Checking the type and shape of y\n",
    "assert type(y) == pd.Series\n",
    "assert y.shape == (1460,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ccf555ce3e71f36d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have `X` and `y`, perform a train-test split ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). Let's say that 40% of the data should be in the test set (60% in the train set), and the random state should be 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a2f9098bf45328b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step1.2\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform train-test split. Replace None for test_size and random_state!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shapes\n",
    "# (If this fails, make sure you specified the appropriate test_size)\n",
    "assert X_train.shape == (876, 33)\n",
    "assert y_train.shape == (876,)\n",
    "\n",
    "# Checking the contents\n",
    "# (If this fails, make sure you specified the appropriate random_state)\n",
    "assert X_train.iloc[100][\"YearBuilt\"] == 1947\n",
    "assert y_train.iloc[100] == 110000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2324103f0af706e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Preprocess Data\n",
    "\n",
    "### Ridge Regression Recap\n",
    "\n",
    "We are going to use a `Ridge` regression, which adds a penalty term to the square of the magnitude of the coefficients.\n",
    "\n",
    "In other words, whereas an ordinary least squares regression uses this cost function:\n",
    "\n",
    "$$ \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\n",
    "\n",
    "...where $n$ is the number of rows in the dataset, $y$ is the target value, $k$ is the number of features in the dataset, $m$ is the slope parameter (coefficient) we are trying to find, $x$ is the value of the feature, and $b$ is the intercept...\n",
    "\n",
    "...a ridge regression uses this cost function:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\n",
    "\n",
    "The difference is the $\\lambda \\sum_{j=1}^p m_j^2$ at the end, where $\\lambda$ is a _hyperparameter_ that we specify, which is multiplied by the coefficients. **The goal of fitting a model is finding $m$ values to *minimize* the cost function**, so a larger $\\lambda$ means more of a penalty on high coefficients. This is a form of *regularization* that should help with overfitting.\n",
    "\n",
    "### Scaling\n",
    "\n",
    "Ridge regression, which uses L2 norm regularization, means that feature values need to be standardized so that some values aren't penalized \"unfairly\". So let's go ahead and apply a `StandardScaler` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) to the entire feature set, fitting and transforming `X_train` and transforming `X_test`.\n",
    "\n",
    "Create new variables `X_train_scaled` and `X_test_scaled` which are the scaled versions of `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec2da7571019a17a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step2\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on X_train and transform X_train\n",
    "scaler.fit(X_train)\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# scaler should be a StandardScaler\n",
    "assert type(scaler) == StandardScaler\n",
    "# scaler should be fitted\n",
    "assert type(scaler.mean_) == np.ndarray\n",
    "\n",
    "# X_train_scaled should have the same shape\n",
    "# as X_train but with different contents\n",
    "assert X_train_scaled.shape == X_train.shape\n",
    "assert X_train_scaled[0][0] != X_train.iloc[0].iloc[0]\n",
    "\n",
    "# Same goes for X_test_scaled\n",
    "assert X_test_scaled.shape == X_test.shape\n",
    "assert X_test_scaled[0][0] != X_test.iloc[0].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b1011f4acfaeec3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Fit a `Ridge` Model\n",
    "\n",
    "Now that we have our preprocessed data, fit a `Ridge` regression model ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)).\n",
    "\n",
    "When instantiating the model, specify an `alpha` (regularization penalty) of 100, a `solver` of `\"sag\"` (stochastic average gradient descent), and a `random_state` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45802a6f1fb065c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=100, random_state=1, solver='sag')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CodeGrade step3\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import Ridge model from scikit-learn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Instantiate the model\n",
    "model = Ridge(alpha=100, solver='sag', random_state=1)\n",
    "\n",
    "# Fit the model on the scaled training data\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model should be a Ridge regressor\n",
    "assert type(model) == Ridge\n",
    "\n",
    "# model should be fitted\n",
    "assert type(model.coef_) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-593a8cb979622402",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Evaluate the Model Performance\n",
    "\n",
    "First, use the fitted model to generate `SalePrice` predictions for both the train and the test data. These variables should be called `y_pred_train` for the training data and `y_pred_test` for the testing data.\n",
    "\n",
    "Make sure you use the scaled versions of the data!\n",
    "\n",
    "We will use these predictions to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bd372462ff071aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150859.10061779, 298850.20458046, 116587.16406088, 177796.45848813,\n",
       "       295211.51258219,  49168.9704058 , 223611.59997064, 158065.83405109,\n",
       "        47037.89699274, 153156.04079068, 149330.22113682, 114923.62081416,\n",
       "       100901.67302232, 205329.39633042, 195168.97807706, 141248.62445603,\n",
       "       212372.66198895, 133747.46055101, 120953.56896699, 227540.50457688,\n",
       "       191069.73000159, 213090.93231262, 194818.59640511, 135458.64509133,\n",
       "       211003.34837354, 153848.03753136, 196490.32809463,  91339.48069502,\n",
       "       188887.71525575, 184579.62819594, 121006.12784143, 271367.02861235,\n",
       "       229542.57439339,  85396.74993294, 269104.80976266, 163522.36128403,\n",
       "       149026.57365114, 217545.74677371, 301008.97315562,  90069.68889194,\n",
       "       144303.17990197, 247875.6845652 , 104969.56110242, 283820.32545159,\n",
       "       132530.22738448, 129075.57562742, 104707.88945051, 127246.72287087,\n",
       "       349609.85586922, 126636.26581982, 104986.86549366, 220968.25581687,\n",
       "        99994.28832732, 308744.75470667, 162748.40525797, 242580.11592753,\n",
       "       229185.38130283, 154510.04805786, 164012.85492778, 117405.04565069,\n",
       "        62373.54539608, 147311.08160043, 278356.52519337, 249011.41505014,\n",
       "       268159.58846671, 218454.33246886,  93686.2504693 , 303849.05059563,\n",
       "       100051.9713222 , 164530.60256962, 125236.9087456 , 116960.21426432,\n",
       "        82656.15677254,  63727.85695376, 361403.66572447, 187649.84111306,\n",
       "       284093.67845125, 288754.59956608, 122540.34649108, 112087.86975499,\n",
       "       126661.17404445,  92380.27937179, 115355.32249274, 105507.54615528,\n",
       "       163250.38464394, 115877.77636683, 267677.065046  , 215051.10600708,\n",
       "       157984.02241905, 208667.10216631, 161584.97889174, 153718.91194472,\n",
       "       122471.02505412, 251422.40706326,  91685.2901353 , 189125.49454596,\n",
       "       191815.78301315, 184108.34874088, 213786.69405277, 257748.29881007,\n",
       "       200150.7651148 , 224961.49762409, 274542.62204286, 153958.12432944,\n",
       "       175200.37143534, 194058.05571802, 168375.50745994, 234377.83033711,\n",
       "       136255.92035825, 190375.08424016,  22179.63575977, 104796.00701529,\n",
       "       143621.74888804, 128817.29381423, 218654.88596385, 128060.98385462,\n",
       "        92714.65230835, 121986.98506142, 152754.78690429, 276418.38917898,\n",
       "       150480.87255774, 157208.76952123, 190875.71835896, 193723.88083236,\n",
       "       200906.48751997, 123504.74364162, 233186.30579319,  89724.67881559,\n",
       "       143563.61228349, 216370.89417509, 196928.53766701, 311004.87462577,\n",
       "       202927.31148432, 121594.97341204,  33091.56109333, 311762.39056703,\n",
       "       306209.24355855, 124108.69274068, 238885.45860613, 412107.42576181,\n",
       "       299462.30908123, 125242.62638575, 189828.9964046 , 171123.43199093,\n",
       "       126075.76477533, 123118.21142181, 206560.58784833, 198663.05577733,\n",
       "       117647.03091631,  44097.9084296 ,  89039.59862297, 158502.5914769 ,\n",
       "       244546.28650843, 162217.43824942,  65959.12645565, 112433.43824542,\n",
       "       142324.13303655, 154125.05447035,  78643.73338469, 142939.1097946 ,\n",
       "       211543.27676247, 158040.14196945, 278989.08496128, 147458.66080578,\n",
       "       114443.75303754, 101178.94881076, 236094.49792867, 291396.51669651,\n",
       "       335034.58923792, 216134.93341282, 323063.97494416,  64342.57600901,\n",
       "       105808.45950168, 170642.67222736, 276897.92519998, 127367.21968204,\n",
       "       132983.17784632, 220178.39521656, 126684.24614034, 153312.08018115,\n",
       "       163025.01834139, 107995.97300811, 121951.07370944, 167646.16956547,\n",
       "       262195.13267236, 175571.90012536, 283521.20803385, 235104.66041731,\n",
       "       197050.78867166,  71793.77312992, 140585.73518118,  97137.77981033,\n",
       "       147439.70146351,  77317.1370059 , 198716.70018669, 193169.95658248,\n",
       "       213540.38553892,  73118.94993256, 220026.40717928, 118844.6815307 ,\n",
       "       249801.79711163, 207645.21827755, 117234.22585689, 287292.3301455 ,\n",
       "       209329.74952941, 111673.25414041, 241451.17407002, 136466.30114211,\n",
       "       163818.09926481,  99807.32208043, 258353.67862473, 171142.76756597,\n",
       "       105961.65833145, 155507.90506303, 219096.95577039, 249762.43112321,\n",
       "       204892.48993347, 134795.20335226, 111445.35381799, 132636.96555117,\n",
       "       149232.06011451, 230931.240178  , 197167.8260057 ,  91883.00754079,\n",
       "       236578.62727186, 166010.70943709,  92656.59673048,  87297.28847116,\n",
       "       169100.8144549 ,  89905.03756229, 100931.03099631, 184748.58567954,\n",
       "       131208.82632412, 106931.36861046, 252597.07787734, 154234.39757573,\n",
       "       216060.33192702, 164178.05132001, 249715.32996456, 132503.73410352,\n",
       "       136149.64835759, 261256.02325204, 234404.42915429, 346337.07500876,\n",
       "       189828.34586344,  90323.98305368, 136848.01984246, 196912.25807295,\n",
       "       145498.95413817,  89287.69483239, 182837.93709781, 196044.32892022,\n",
       "       138839.88741411,  74527.3380724 , 150959.32940097, 153164.65495863,\n",
       "       128001.94669392, 114297.06910835, 179356.27781393, 262306.20077649,\n",
       "       285732.87369745, 189038.33807819, 126401.34595952, 252259.596238  ,\n",
       "       288481.93275513, 226443.69310376, 176006.01622546, 154955.00366985,\n",
       "       111384.40372986, 199105.66325672, 338193.83871932, 225686.85266278,\n",
       "       230352.7882862 ,  81739.0030478 ,  94703.85832173, 143132.37743028,\n",
       "       181435.22602998, 252612.3019966 , 208811.90929769, 138395.85673559,\n",
       "       217675.72328868,  69467.51070819, 195747.7024747 , 116118.41122778,\n",
       "       281324.77907856, 181182.08444308, 219166.0760237 , 128830.05731498,\n",
       "       240533.08117628, 218213.828936  , 107113.14328657, 102447.67194514,\n",
       "       135536.30794513, 186762.23056017,  75680.91837073, 158016.32316641,\n",
       "       127982.43013496, 147444.78192501, 185902.16016805, 116016.78958424,\n",
       "       198056.13310493, 225788.17237597, 122237.62478628, 139831.83009226,\n",
       "       206144.24021737, 242838.09235329, 165785.19807887, 211971.34393358,\n",
       "       223971.7838364 , 138150.26857557, 163334.04042511, 222463.25309728,\n",
       "        82858.06843984, 221894.90005519, 125352.38902428, 192810.7363487 ,\n",
       "       206062.16620294, 168095.35683051, 256307.89471971,  63306.41291774,\n",
       "       208265.26786821, 131383.40272558, 123117.48750989,  87336.4897507 ,\n",
       "       219765.98625332, 182945.30973044, 135244.97047761, 218586.21654573,\n",
       "       190348.59268007,  83068.12168455, 165022.66295813, 158678.36804948,\n",
       "       129391.64546682, 241678.99829188, 168313.36946444, 126558.28283869,\n",
       "       154919.40546588,  56088.29851629,  89267.85025095, 198654.9628648 ,\n",
       "       211030.44742798, 146637.70265224,  99187.58136014, 202371.07226186,\n",
       "       240855.67334494, 324164.86749993, 302884.43920136, 124928.57504078,\n",
       "       256592.15202004, 112204.95685721, 298992.12240027, 319912.87548675,\n",
       "       246757.93737414, 193520.91597277, 249251.62682893, 107562.4466694 ,\n",
       "       116879.41634022,  85318.88247846, 214551.60048241, 306362.57908304,\n",
       "       194197.27622934, 125692.81910917, 249974.65710701, 229031.57947956,\n",
       "       123738.55145178, 204553.51707887, 102084.948057  , 119239.79995616,\n",
       "       130225.37625378, 147336.68527432, 114834.71153088, 194002.79983181,\n",
       "       125751.78396711, 213490.72210649, 186612.12938785, 129233.68906924,\n",
       "       198863.39117076, 202617.62939608,  98623.47710656, 223891.85584719,\n",
       "       155463.98691298, 191822.03864575, 209410.71216863, 156153.74142013,\n",
       "       121100.37739862, 241028.92454433, 152619.31979065, 139830.01257034,\n",
       "       264416.88431633, 193436.15706356, 114515.58063318, 175011.72728414,\n",
       "       139874.41003492, 305008.34376692, 164029.7543747 , 344184.24710808,\n",
       "       141017.99081702, 179244.09072917, 181133.58960265, 157958.77200508,\n",
       "       185718.50695341, 322237.65615725,  98383.95116177, 161468.83055327,\n",
       "       130421.55298   ,  77292.83706824, 213572.40473028, 138302.286108  ,\n",
       "       236057.45876816, 215164.78034688, 239763.44152745, 229541.36358647,\n",
       "        61838.93878684, 284452.14280868, 128502.93004936, 318439.09008638,\n",
       "       107640.61645385, 311028.10001645, 264227.66343582, 121932.79765839,\n",
       "       226525.22444295, 234135.32217248, 214911.6246865 , 185245.67900541,\n",
       "       249193.49788798,  99391.25898891, 215491.23394287, 202768.76843962,\n",
       "       206555.07250402, 227287.97144004, 110042.30952372, 154623.32028995,\n",
       "       191344.15661345, 134291.32714193, 111728.6263063 , 205611.93116524,\n",
       "       146705.57737833, 347082.49767717, 205315.72656502, 132515.22348226,\n",
       "       205624.38276075, 280805.75648279, 219973.30313765, 164286.72348911,\n",
       "       216553.10218757, 128862.78084811, 192253.82260206, 228009.71509673,\n",
       "       126449.41131449, 177972.93349544,  97425.28926825, 171239.2489716 ,\n",
       "       270164.63285249, 143410.76596035, 109815.56694988, 216925.74855327,\n",
       "       302900.75019319, 311003.47584534, 253808.8368959 , 161998.18635803,\n",
       "       112637.36941662,  97190.71592674, 167695.52814843, 214102.09684071,\n",
       "       179416.66348477, 347972.05542546, 236514.08179926, 210773.01305173,\n",
       "       103216.55856984, 266117.85541829, 114605.92455585, 116797.00472988,\n",
       "       104612.79437094, 118700.92431556, 226626.86113973, 257632.54820193,\n",
       "       116308.66841727, 119945.30923426, 111939.47442258, 233261.51527126,\n",
       "       126660.91749344, 209684.91611221, 151810.59558092, 118057.47997896,\n",
       "       231879.42087385, 150730.70282595, 179773.60030089,  98080.83419142,\n",
       "       180780.42123562, 327222.32903696, 233520.62100659, 208297.42942456,\n",
       "       209996.95936322, 284529.18687196, 194163.41557477, 196413.25127173,\n",
       "       230615.13793925,  88652.74464479,  49223.94250064, 197615.10017116,\n",
       "       345485.3905307 ,  69226.86085429, 103798.27293613, 152638.59049366,\n",
       "       197116.8311674 , 148301.08100032, 241678.29791035, 161765.74758308,\n",
       "       228372.37881052, 108321.95630643, 198258.64449425,  73195.77281841,\n",
       "       157638.3641348 , 259509.26355164, 122608.91846463, 150034.04878466,\n",
       "       242378.56504799,  93233.32504966, 246050.52888091, 179750.31578868,\n",
       "       138731.43948411, 198261.94100724, 224884.45009048,  94693.91458024,\n",
       "       213104.70880875, 208899.64624646, 182771.75968544, 129240.09633094,\n",
       "       214090.30755227, 149833.82320879, 264380.05584121, 280246.27481503,\n",
       "       246831.08457595, 180791.22509543, 284938.477381  , 182369.50105205,\n",
       "       145071.69713848, 136439.69591371, 206683.9426837 , 174264.9715046 ,\n",
       "       170058.99326652, 150910.59943046, 277925.88329537, 262770.25937905,\n",
       "       270655.81700544, 250628.0511551 , 148207.72431038, 212882.0154591 ,\n",
       "       170553.94661936, 123653.64533509, 324360.88874166, 150646.4054026 ,\n",
       "       279297.74170799, 103984.68666858, 149548.19521632, 195015.27688951,\n",
       "        21302.10436185, 124263.50596498, 207662.54155499, 240822.6473031 ,\n",
       "       160866.23728585, 264892.82535636, 136457.98955904, 197327.62881544,\n",
       "       317063.90077413, 156167.46547984, 302400.52935769, 189086.92554207,\n",
       "       235864.50239517, 181180.39711913, 173353.44415335,  56813.92917715,\n",
       "       228150.10433022,  90496.36804127, 130823.68805979, 179283.70104274,\n",
       "       251751.2796114 , 162641.83372131, 262132.1328035 , 253515.75796229,\n",
       "       155166.88646774, 206194.89494389, 323269.41692436, 128572.352527  ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CodeGrade step4.1\n",
    "# Replace None with appropriate code\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "y_pred_train\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both should be NumPy arrays\n",
    "assert type(y_pred_train) == np.ndarray\n",
    "assert type(y_pred_test) == np.ndarray\n",
    "\n",
    "# Should have the same shapes as y_train and y_test, respectively\n",
    "assert y_pred_train.shape == y_train.shape\n",
    "assert y_pred_test.shape == y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb5aa46c99fc351c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, use those predicted values to evaluate the model in terms of both:\n",
    "\n",
    "* RMSE, using `mean_squared_error` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)) with `squared=False`\n",
    "* R-squared, using `r2_score` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html))\n",
    "\n",
    "Apply these to both the train and test datasets. We have already imported the necessary functions; you just need to call the functions and pass in the appropriate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7689e4d208bcbff6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE\n",
      "Train: 33910.83631269176 \t Test: 39213.661587625924\n",
      "\n",
      "R-squared\n",
      "Train: 0.7974405308436691 \t Test: 0.7878321566714682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CodeGrade step4.2\n",
    "# Replace None with appropriate code\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_pred_train,squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_pred_test,squared=False)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\"\"\n",
    "RMSE\n",
    "Train: {train_rmse} \\t Test: {test_rmse}\n",
    "\n",
    "R-squared\n",
    "Train: {train_r2} \\t Test: {test_r2}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE scores should be floating point numbers\n",
    "assert type(train_rmse) == np.float64 or type(train_rmse) == float\n",
    "assert type(test_rmse) == np.float64 or type(test_rmse) == float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared scores should be floating point numbers\n",
    "assert type(train_r2) == np.float64 or type(train_r2) == float\n",
    "assert type(test_r2) == np.float64 or type(test_r2) == float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f0a2420444cac43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Interpret the Model Performance\n",
    "\n",
    "Here's we'll focus on RMSE metrics, since those can be represented as \"the average error in the price prediction\".\n",
    "\n",
    "Recall that the purpose of using regularization (e.g. a `Ridge` model) is to reduce overfitting.\n",
    "\n",
    "Let's say that we previously used a basic ordinary least squares regression model, and we got RMSE scores of `$33,633.14` on the training data and `$39,255.80` on the test data. A full comparison of scores is in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6d0c33512bd6e89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_b8ca52fa_f653_11ed_8f83_f0b61ee986c0\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Train RMSE</th>        <th class=\"col_heading level0 col1\" >Test RMSE</th>    </tr>    <tr>        <th class=\"index_name level0\" >Model</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b8ca52fa_f653_11ed_8f83_f0b61ee986c0level0_row0\" class=\"row_heading level0 row0\" >Linear Regression</th>\n",
       "                        <td id=\"T_b8ca52fa_f653_11ed_8f83_f0b61ee986c0row0_col0\" class=\"data row0 col0\" >$33,633.14</td>\n",
       "                        <td id=\"T_b8ca52fa_f653_11ed_8f83_f0b61ee986c0row0_col1\" class=\"data row0 col1\" >$39,255.80</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b8ca52fa_f653_11ed_8f83_f0b61ee986c0level0_row1\" class=\"row_heading level0 row1\" >Ridge Regression</th>\n",
       "                        <td id=\"T_b8ca52fa_f653_11ed_8f83_f0b61ee986c0row1_col0\" class=\"data row1 col0\" >$33,910.84</td>\n",
       "                        <td id=\"T_b8ca52fa_f653_11ed_8f83_f0b61ee986c0row1_col1\" class=\"data row1 col1\" >$39,213.66</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19ae5d35b80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "scores = pd.DataFrame([\n",
    "    {\"Model\": \"Linear Regression\", \"Train RMSE\": 33633.14, \"Test RMSE\": 39255.80},\n",
    "    {\"Model\": \"Ridge Regression\", \"Train RMSE\": 33910.84, \"Test RMSE\": 39213.66},\n",
    "])\n",
    "scores.set_index(\"Model\", inplace=True)\n",
    "scores.style.format(\"${:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a8a4cc0ff171ee2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Was our strategy of using a `Ridge` model to reduce overfitting successful? Which model is better?\n",
    "\n",
    "Assign the variable `best_model_name` to either `\"Linear Regression\"` or `\"Ridge Regression\"`.\n",
    "\n",
    "Recall that this is a predictive modeling context, so when we are defining the \"best\" model, we are interested in the model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42f97ee6482596fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step5\n",
    "# Replace None with appropriate code\n",
    "best_model_name = 'Linear Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be \"Linear Regression\" or \"Ridge Regression\"\n",
    "assert best_model_name in [\"Linear Regression\", \"Ridge Regression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
